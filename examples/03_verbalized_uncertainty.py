"""
TECHNIQUE 3: VERBALIZED UNCERTAINTY
From SURVEY.md Â§ 4.3

Principle: Train model to explicitly express uncertainty via:
- Prompt engineering with few-shot examples
- Fine-tuning on labeled "I don't know" responses

Two sub-methods:
- Prompt Engineering: Few-shot teaching
- Fine-tuning with Labels: Train on abstention examples
"""

import torch
from utils import load_tokenizer_and_model


class VerbalizedUncertainty:
    """
    Implements verbalized uncertainty abstention.
    Mirrors paper's principle: Train model to verbalize "I don't know".
    """
    
    def __init__(self, model_name="mistralai/Mistral-7B-Instruct-v0.1"):
        """Load model and tokenizer"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer, self.model = load_tokenizer_and_model(model_name)
        
        # Uncertainty keywords (used for detection)
        self.uncertainty_phrases = [
            "i don't know",
            "i cannot determine",
            "cannot find",
            "not mentioned",
            "insufficient information",
            "unclear from the context",
            "no information",
            "cannot say",
            "i'm uncertain",
            "i have no information"
        ]
    
    def method_1_prompt_engineering(self, question, context=""):
        """
        METHOD 1: Prompt Engineering with Few-Shot
        
        Paper principle (Â§4.3.1):
        Few-shot examples teach model to say "I don't know" when appropriate.
        Bartolo et al. (2020): Few-shot prompting increases abstention accuracy.
        
        Args:
            question: Query string
            context: Optional background
        
        Returns:
            decision_dict: Response, contains_abstention_signal, decision
        """
        
        # Few-shot template from paper
        few_shot_examples = """Example 1:
Q: What is the capital of France?
Context: France is in Western Europe. Its capital is Paris.
A: The capital of France is Paris.

Example 2:
Q: Who invented the quantum computer?
Context: No relevant information provided.
A: I don't know. The context does not contain information about who invented the quantum computer.

Example 3:
Q: What color is water?
Context: The sky is blue.
A: I don't know. The provided context is about the sky, not water."""
        
        # Construct prompt with few-shot examples
        prompt = f"""{few_shot_examples}

Example 4:
Q: {question}
Context: {context if context else "No context provided."}
A:"""
        
        # Generate response
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            output = self.model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.1,
                do_sample=False
            )
        
        response_ids = output[0, inputs.input_ids.shape[-1]:]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()
        
        # Check if model expressed uncertainty
        response_lower = response.lower()
        contains_abstention = any(phrase in response_lower for phrase in self.uncertainty_phrases)
        
        # Decision
        if contains_abstention:
            decision = "ABSTAIN: Model expressed uncertainty"
        else:
            decision = response
        
        return {
            "response": response,
            "contains_abstention_signal": contains_abstention,
            "method": "Prompt Engineering",
            "decision": decision
        }
    
    def method_2_uncertainty_extraction(self, question, context="", confidence_threshold=0.6):
        """
        METHOD 2: Uncertainty Token Detection
        
        Paper principle (Â§4.3.2):
        Fine-tuning on SQuAD2/Abstain-QA teaches model "I don't know" responses.
        Here: Detect uncertainty via logit analysis on special tokens.
        
        Args:
            question: Query string
            context: Optional background
            confidence_threshold: Threshold for confidence
        
        Returns:
            decision_dict: Response, uncertainty_score, decision
        """
        
        # Create prompt requesting model to indicate confidence
        prompt = f"""Question: {question}
Context: {context if context else "No context provided."}

Provide your best answer. If uncertain, say "I don't know".
Answer:"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            output = self.model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.1,
                output_scores=True,
                return_dict_in_generate=True
            )
        
        response_ids = output.sequences[0, inputs.input_ids.shape[-1]:]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()
        
        # Calculate uncertainty score from token probabilities
        if output.scores:
            token_probs = [score.max().item() for score in output.scores]
            avg_confidence = sum(token_probs) / len(token_probs)
        else:
            avg_confidence = 1.0
        
        uncertainty_score = 1.0 - avg_confidence
        
        # Check for uncertainty phrases
        response_lower = response.lower()
        has_explicit_uncertainty = any(phrase in response_lower for phrase in self.uncertainty_phrases)
        
        # Combined uncertainty signal
        final_uncertainty = (uncertainty_score + float(has_explicit_uncertainty)) / 2
        
        # Decision
        if final_uncertainty > (1 - confidence_threshold) or has_explicit_uncertainty:
            decision = f"ABSTAIN: High uncertainty signal ({final_uncertainty:.2f})"
        else:
            decision = response
        
        return {
            "response": response,
            "token_confidence": avg_confidence,
            "explicit_uncertainty_signals": has_explicit_uncertainty,
            "combined_uncertainty_score": final_uncertainty,
            "decision": decision,
            "method": "Uncertainty Extraction"
        }
    
    def method_3_confidence_statement(self, question, context=""):
        """
        METHOD 3: Confidence Statement Parsing
        
        Paper principle (Â§4.3):
        Request model to append confidence statement; parse to abstain.
        
        Args:
            question: Query string
            context: Optional background
        
        Returns:
            decision_dict: Response, confidence_level, decision
        """
        
        # Prompt with explicit confidence request
        prompt = f"""Question: {question}
Context: {context if context else "No context provided."}

Answer the question and rate your confidence (high/medium/low):
Answer:"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            output = self.model.generate(
                **inputs,
                max_new_tokens=80,
                temperature=0.1,
                do_sample=False
            )
        
        response_ids = output[0, inputs.input_ids.shape[-1]:]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()
        
        # Parse confidence level
        response_lower = response.lower()
        
        if "low" in response_lower or "not sure" in response_lower or "uncertain" in response_lower:
            confidence_level = "low"
            confidence_score = 0.3
        elif "medium" in response_lower or "somewhat" in response_lower:
            confidence_level = "medium"
            confidence_score = 0.6
        else:
            confidence_level = "high"
            confidence_score = 0.9
        
        # Decision based on confidence level
        if confidence_level == "low":
            decision = "ABSTAIN: Model reports low confidence"
        else:
            decision = response.split("confidence")[0].strip() if "confidence" in response_lower else response
        
        return {
            "response": response,
            "parsed_confidence_level": confidence_level,
            "confidence_score": confidence_score,
            "decision": decision,
            "method": "Confidence Statement Parsing"
        }


def main():
    """Test verbalized uncertainty on sample questions"""

    # Header
    print("\n" + "â•" * 80)
    print("ğŸ¯ VERBALIZED UNCERTAINTY DEMONSTRATION")
    print("   From SURVEY Â§ 4.3: Verbalized Uncertainty Methods")
    print("â•" * 80)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WHAT IS VERBALIZED UNCERTAINTY?                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Core Idea: Train/prompt the model to EXPLICITLY express uncertainty       â”‚
â”‚  using natural language like "I don't know" or "I'm not sure".             â”‚
â”‚                                                                             â”‚
â”‚  From Paper (Â§ 4.3):                                                        â”‚
â”‚    "Verbalized uncertainty trains models to produce linguistic markers     â”‚
â”‚     of uncertainty rather than relying on implicit confidence scores"      â”‚
â”‚                                                                             â”‚
â”‚  Key Insight:                                                               â”‚
â”‚    Confidence-based: Infer uncertainty from token probabilities            â”‚
â”‚    Verbalized: Model directly SAYS when it's uncertain                     â”‚
â”‚                                                                             â”‚
â”‚  Two Main Approaches:                                                       â”‚
â”‚    1. Prompt Engineering: Few-shot examples teach "I don't know" patterns  â”‚
â”‚    2. Fine-tuning: Train on datasets with explicit abstention labels       â”‚
â”‚       (e.g., SQuAD 2.0, Abstain-QA)                                        â”‚
â”‚                                                                             â”‚
â”‚  Referenced Works:                                                          â”‚
â”‚    â€¢ Bartolo et al. (2020): Few-shot prompting for abstention              â”‚
â”‚    â€¢ Rajpurkar et al. (2018): SQuAD 2.0 unanswerable questions             â”‚
â”‚                                                                             â”‚
â”‚  Three Sub-Methods We'll Test:                                              â”‚
â”‚    1. Prompt Engineering (Few-Shot): Teach via examples                    â”‚
â”‚    2. Uncertainty Extraction: Detect uncertainty phrases + logits          â”‚
â”‚    3. Confidence Statement Parsing: Model self-reports confidence          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

    print("ğŸ“š Reference: SURVEY Â§ 4.3 - Verbalized Uncertainty\n")

    print("â³ Loading Mistral-7B-Instruct...")
    abstainer = VerbalizedUncertainty()
    print("âœ“ Model loaded\n")
    
    test_cases = [
        ("What is the capital of France?", "France is in Western Europe and its capital is Paris."),
        ("Who invented the quantum computer?", ""),  # Unanswerable
        ("What did the abstract say?", "No abstract was provided."),
    ]
    
    # ==================== METHOD 1 ====================
    print("\n" + "â•" * 80)
    print("METHOD 1: PROMPT ENGINEERING WITH FEW-SHOT")
    print("â•" * 80)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOW FEW-SHOT PROMPT ENGINEERING WORKS (SURVEY Â§ 4.3.1)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Principle: Include examples in the prompt that demonstrate when and       â”‚
â”‚  how to say "I don't know".                                                â”‚
â”‚                                                                             â”‚
â”‚  From Paper:                                                                â”‚
â”‚    "Bartolo et al. (2020) showed that few-shot examples significantly      â”‚
â”‚     improve abstention accuracy by teaching the model appropriate          â”‚
â”‚     refusal patterns"                                                       â”‚
â”‚                                                                             â”‚
â”‚  Few-Shot Template Used:                                                    â”‚
â”‚    Example 1: Q with context â†’ Direct answer                               â”‚
â”‚    Example 2: Q without info â†’ "I don't know. The context does not..."     â”‚
â”‚    Example 3: Q with irrelevant context â†’ "I don't know..."                â”‚
â”‚    New question: Model follows the pattern                                 â”‚
â”‚                                                                             â”‚
â”‚  Key Features:                                                              â”‚
â”‚    â€¢ No training required (prompt-only)                                    â”‚
â”‚    â€¢ Works with any instruction-tuned model                                â”‚
â”‚    â€¢ Examples teach the format of uncertainty expression                   â”‚
â”‚                                                                             â”‚
â”‚  Detection: Scan response for uncertainty phrases like "I don't know"      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

    print("â–¶ Running Method 1 with few-shot prompt...")
    print("  Detecting uncertainty phrases in response\n")

    for question, context in test_cases[:1]:
        result = abstainer.method_1_prompt_engineering(question, context)
        print(f"â— Q: {question}")
        print(f"  Context: {context[:50]}..." if context else "  Context: None")
        print(f"  Response: {result['response'][:80]}...")
        print(f"  Contains abstention signal: {result['contains_abstention_signal']}")
        if result['decision'].startswith("ABSTAIN"):
            print(f"  â†’ ABSTAIN (model expressed uncertainty)")
        else:
            print(f"  â†’ ANSWER: {result['decision'][:50]}...")
        print()

    print("â”€" * 80)
    print("ğŸ“Š METHOD 1 OBSERVATION:")
    print("   Few-shot examples teach the model WHEN to say 'I don't know'.")
    print("   The model learns the pattern from examples, no fine-tuning needed.")
    print("   Detection relies on scanning for uncertainty phrases.")
    print("â”€" * 80)
    
    # ==================== METHOD 2 ====================
    print("\n" + "â•" * 80)
    print("METHOD 2: UNCERTAINTY EXTRACTION")
    print("â•" * 80)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOW UNCERTAINTY EXTRACTION WORKS (SURVEY Â§ 4.3.2)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Principle: Combine EXPLICIT signals (uncertainty phrases) with            â”‚
â”‚  IMPLICIT signals (token probabilities) for robust detection.              â”‚
â”‚                                                                             â”‚
â”‚  From Paper:                                                                â”‚
â”‚    "Fine-tuning on SQuAD 2.0 and Abstain-QA teaches models to produce      â”‚
â”‚     'I don't know' responses for unanswerable questions"                   â”‚
â”‚                                                                             â”‚
â”‚  Combined Signals:                                                          â”‚
â”‚    1. Explicit: Does response contain "I don't know", "unclear", etc.?     â”‚
â”‚    2. Implicit: Are token probabilities low (model hesitating)?            â”‚
â”‚    3. Combined: (explicit_uncertainty + implicit_uncertainty) / 2          â”‚
â”‚                                                                             â”‚
â”‚  Why Combine Both:                                                          â”‚
â”‚    â€¢ Explicit alone: Model might say "I don't know" incorrectly            â”‚
â”‚    â€¢ Implicit alone: Low probs might be writing style, not uncertainty     â”‚
â”‚    â€¢ Combined: More robust signal from multiple sources                    â”‚
â”‚                                                                             â”‚
â”‚  Uncertainty Score Formula:                                                 â”‚
â”‚    uncertainty = 1 - avg_token_probability                                 â”‚
â”‚    final = (uncertainty + has_explicit_phrases) / 2                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

    print("â–¶ Running Method 2 on test questions...")
    print("  Combining explicit and implicit uncertainty signals\n")

    for question, context in test_cases:
        result = abstainer.method_2_uncertainty_extraction(question, context)
        print(f"â— Q: {question}")
        print(f"  Response: {result['response'][:60]}...")
        print(f"  Token Confidence: {result['token_confidence']:.3f}")
        print(f"  Has Explicit Uncertainty: {result['explicit_uncertainty_signals']}")
        print(f"  Combined Uncertainty Score: {result['combined_uncertainty_score']:.3f}")
        if result['decision'].startswith("ABSTAIN"):
            print(f"  â†’ ABSTAIN (high uncertainty)")
        else:
            print(f"  â†’ ANSWER: {result['decision'][:40]}...")
        print()

    print("â”€" * 80)
    print("ğŸ“Š METHOD 2 OBSERVATION:")
    print("   Combining explicit + implicit signals is more robust than either alone.")
    print("   A model saying 'I don't know' with low confidence â†’ strong abstention signal.")
    print("   A confident answer without uncertainty phrases â†’ proceed with answer.")
    print("â”€" * 80)
    
    # ==================== METHOD 3 ====================
    print("\n" + "â•" * 80)
    print("METHOD 3: CONFIDENCE STATEMENT PARSING")
    print("â•" * 80)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOW CONFIDENCE STATEMENT PARSING WORKS (SURVEY Â§ 4.3)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Principle: Ask the model to explicitly rate its own confidence,           â”‚
â”‚  then parse that rating to decide whether to abstain.                      â”‚
â”‚                                                                             â”‚
â”‚  Prompt Template:                                                           â”‚
â”‚    "Answer the question and rate your confidence (high/medium/low)"        â”‚
â”‚                                                                             â”‚
â”‚  Parsing Logic:                                                             â”‚
â”‚    â€¢ Response contains "high" or "certain" â†’ confidence = 0.9              â”‚
â”‚    â€¢ Response contains "medium" or "somewhat" â†’ confidence = 0.6           â”‚
â”‚    â€¢ Response contains "low" or "uncertain" â†’ confidence = 0.3             â”‚
â”‚                                                                             â”‚
â”‚  Decision:                                                                  â”‚
â”‚    â€¢ If parsed confidence = "low" â†’ ABSTAIN                                â”‚
â”‚    â€¢ Otherwise â†’ return the answer                                         â”‚
â”‚                                                                             â”‚
â”‚  Key Advantage:                                                             â”‚
â”‚    â€¢ Model self-reports in interpretable terms                             â”‚
â”‚    â€¢ Easy for humans to understand the abstention reason                   â”‚
â”‚    â€¢ Can be calibrated by training on confidence-labeled data              â”‚
â”‚                                                                             â”‚
â”‚  Limitation:                                                                â”‚
â”‚    â€¢ Models may not be well-calibrated in self-assessment                  â”‚
â”‚    â€¢ Confidence statements may be overconfident (like token probs)         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

    print("â–¶ Running Method 3 on test question...")
    print("  Parsing self-reported confidence level\n")

    for question, context in test_cases[:1]:
        result = abstainer.method_3_confidence_statement(question, context)
        print(f"â— Q: {question}")
        print(f"  Response: {result['response'][:80]}...")
        print(f"  Parsed Confidence Level: {result['parsed_confidence_level']}")
        print(f"  Confidence Score: {result['confidence_score']:.2f}")
        if result['decision'].startswith("ABSTAIN"):
            print(f"  â†’ ABSTAIN (low self-reported confidence)")
        else:
            print(f"  â†’ ANSWER: {result['decision'][:50]}...")
        print()

    print("â”€" * 80)
    print("ğŸ“Š METHOD 3 OBSERVATION:")
    print("   Self-reported confidence is interpretable but may not be calibrated.")
    print("   Models often over-report confidence (similar to softmax issue).")
    print("   Works best when combined with other uncertainty signals.")
    print("â”€" * 80)

    # ==================== SUMMARY ====================
    print("\n" + "â•" * 80)
    print("VERBALIZED UNCERTAINTY: KEY INSIGHTS")
    print("â•" * 80)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMPARISON OF VERBALIZED UNCERTAINTY METHODS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Method 1: Few-Shot Prompt Engineering                                      â”‚
â”‚    âœ“ No training required                                                  â”‚
â”‚    âœ“ Works with any instruction-tuned model                                â”‚
â”‚    âœ— Relies on model following examples consistently                       â”‚
â”‚    â†’ Best for: Quick deployment without fine-tuning                        â”‚
â”‚                                                                             â”‚
â”‚  Method 2: Uncertainty Extraction (Explicit + Implicit)                     â”‚
â”‚    âœ“ Robust: combines multiple signals                                     â”‚
â”‚    âœ“ Catches uncertainty even when model doesn't verbalize                 â”‚
â”‚    âœ— Requires access to logits/probabilities                               â”‚
â”‚    â†’ Best for: Maximum detection accuracy                                  â”‚
â”‚                                                                             â”‚
â”‚  Method 3: Confidence Statement Parsing                                     â”‚
â”‚    âœ“ Interpretable: human-readable confidence                              â”‚
â”‚    âœ“ Simple to implement                                                   â”‚
â”‚    âœ— Models may be overconfident in self-assessment                        â”‚
â”‚    â†’ Best for: Systems requiring explainable abstention                    â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  KEY INSIGHT FROM PAPER:                                                    â”‚
â”‚                                                                             â”‚
â”‚  "Verbalized uncertainty is more interpretable than confidence scores,     â”‚
â”‚   but requires either careful prompting or fine-tuning on abstention       â”‚
â”‚   datasets like SQuAD 2.0 to achieve reliable performance"                 â”‚
â”‚                                                                             â”‚
â”‚  The key is teaching the model WHEN to say "I don't know" -               â”‚
â”‚  either through examples (few-shot) or training data (fine-tuning).        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

    print("â•" * 80)
    print("END OF VERBALIZED UNCERTAINTY DEMONSTRATION")
    print("â•" * 80 + "\n")


if __name__ == "__main__":
    main()
